{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "\"\"\"\n",
    "# 智能搜索引擎\n",
    "此模块实现了一个混合搜索引擎，支持结构化查询和向量相似度搜索。\n",
    "适配预处理后的统一数据格式，提供精确和模糊搜索功能。\n",
    "\n",
    "主要功能：\n",
    "- 结构化数据查询\n",
    "- 向量相似度搜索\n",
    "- 混合搜索结果排序\n",
    "- 智能结果增强\n",
    "\"\"\"\n",
    "\n",
    "# %% [1. 环境准备]\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, List, Union, Tuple\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 设置环境变量\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# %% [2. 常量定义]\n",
    "DEFAULT_MODEL = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "DEFAULT_DB_PATH = \"unified_storage.duckdb\"\n",
    "SEARCH_TOP_K = 5\n",
    "\n",
    "# %% [3. 数据库初始化]\n",
    "def setup_database(db_path: str = DEFAULT_DB_PATH):\n",
    "    \"\"\"初始化数据库表结构\"\"\"\n",
    "    db = duckdb.connect(db_path)\n",
    "    \n",
    "    # 创建统一数据表\n",
    "    db.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS unified_data (\n",
    "            _record_id VARCHAR PRIMARY KEY,  \n",
    "            _file_path VARCHAR,             \n",
    "            _file_name VARCHAR,             \n",
    "            _file_type VARCHAR,             \n",
    "            _processed_at TIMESTAMP,        \n",
    "            _sub_id INTEGER,                \n",
    "            data JSON,                      \n",
    "            vector DOUBLE[]                 \n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    db.close()\n",
    "\n",
    "# %% [4. 搜索引擎核心类]\n",
    "class SearchEngine:\n",
    "    \"\"\"智能搜索引擎核心类\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = DEFAULT_DB_PATH):\n",
    "        \"\"\"初始化搜索引擎\n",
    "        Args:\n",
    "            db_path: DuckDB数据库路径\n",
    "        \"\"\"\n",
    "        self.db = duckdb.connect(db_path)\n",
    "        self.text_model = SentenceTransformer(DEFAULT_MODEL)\n",
    "        self.faiss_index = None\n",
    "        self.vectors_map = {}  # 存储向量ID到源数据的映射\n",
    "        self.load_vectors()\n",
    "\n",
    "    def load_vectors(self):\n",
    "        \"\"\"加载并处理向量数据\"\"\"\n",
    "        try:\n",
    "            # 从unified_data表获取数据\n",
    "            vector_data = self.db.execute(\"\"\"\n",
    "                SELECT _record_id, _file_path, _file_name, _file_type, vector, data\n",
    "                FROM unified_data\n",
    "                WHERE vector IS NOT NULL\n",
    "            \"\"\").fetchall()\n",
    "            \n",
    "            if vector_data:\n",
    "                print(f\"找到 {len(vector_data)} 条向量记录\")\n",
    "                \n",
    "                # 构建向量数据\n",
    "                vectors = []\n",
    "                for idx, record in enumerate(vector_data):\n",
    "                    try:\n",
    "                        # 直接使用向量数据，因为DuckDB已经将其转换为Python列表\n",
    "                        vector = record[4]\n",
    "                        vectors.append(np.array(vector))\n",
    "                        \n",
    "                        # 保存映射关系\n",
    "                        self.vectors_map[len(vectors)-1] = {\n",
    "                            'record_id': record[0],\n",
    "                            'file_path': record[1],\n",
    "                            'file_name': record[2],\n",
    "                            'file_type': record[3],\n",
    "                            'data': record[5]  # 直接使用data字段，不需要JSON解析\n",
    "                        }\n",
    "                    except Exception as e:\n",
    "                        print(f\"处理记录 {record[0]} 时出错: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                if vectors:\n",
    "                    vectors = np.stack(vectors)\n",
    "                    dimension = vectors.shape[1]\n",
    "                    \n",
    "                    # 初始化FAISS索引\n",
    "                    self.faiss_index = faiss.IndexFlatL2(dimension)\n",
    "                    self.faiss_index.add(vectors.astype('float32'))\n",
    "                    \n",
    "                    print(f\"成功加载 {len(vectors)} 个向量\")\n",
    "                else:\n",
    "                    print(\"未找到有效的向量数据\")\n",
    "            else:\n",
    "                print(\"未在数据库中找到向量数据\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"加载向量数据时出错: {str(e)}\")\n",
    "            print(\"将继续运行，但向量搜索功能可能不可用\")\n",
    "\n",
    "    def parse_query(self, query: str) -> Dict:\n",
    "        \"\"\"解析查询字符串\n",
    "        \n",
    "        支持的查询格式:\n",
    "        - 普通查询: \"关键词\"\n",
    "        - 文件类型: \"file:txt\"\n",
    "        - 日期范围: \"date:2023-01-01 to 2023-12-31\"\n",
    "        \n",
    "        Args:\n",
    "            query: 用户输入的查询字符串\n",
    "            \n",
    "        Returns:\n",
    "            Dict: 包含查询类型和内容的字典\n",
    "        \"\"\"\n",
    "        patterns = {\n",
    "            'text': r'^(?!file:|date:).*',\n",
    "            'file': r'file:(\\w+)',\n",
    "            'date': r'date:(\\d{4}-\\d{2}-\\d{2})\\s+to\\s+(\\d{4}-\\d{2}-\\d{2})'\n",
    "        }\n",
    "        \n",
    "        for query_type, pattern in patterns.items():\n",
    "            if match := re.match(pattern, query):\n",
    "                return {\n",
    "                    'type': query_type,\n",
    "                    'content': match.groups()[0] if match.groups() else query\n",
    "                }\n",
    "        \n",
    "        return {'type': 'text', 'content': query}\n",
    "\n",
    "    def execute_structured_query(self, parsed_query: Dict) -> pd.DataFrame:\n",
    "        \"\"\"执行结构化数据查询\"\"\"\n",
    "        query_type = parsed_query['type']\n",
    "        content = parsed_query['content']\n",
    "        \n",
    "        try:\n",
    "            if query_type == 'text':\n",
    "                return self.db.execute(f\"\"\"\n",
    "                    SELECT _record_id, _file_path, _file_name, _file_type, \n",
    "                           _processed_at, data\n",
    "                    FROM unified_data\n",
    "                    WHERE data::TEXT ILIKE '%{content}%'\n",
    "                    ORDER BY _processed_at DESC\n",
    "                    LIMIT 10\n",
    "                \"\"\").fetchdf()\n",
    "                \n",
    "            elif query_type == 'file':\n",
    "                return self.db.execute(f\"\"\"\n",
    "                    SELECT _record_id, _file_path, _file_name, _file_type, \n",
    "                           _processed_at, data\n",
    "                    FROM unified_data\n",
    "                    WHERE _file_type = '{content}'\n",
    "                    ORDER BY _processed_at DESC\n",
    "                    LIMIT 10\n",
    "                \"\"\").fetchdf()\n",
    "                \n",
    "            elif query_type == 'date':\n",
    "                start_date, end_date = content\n",
    "                return self.db.execute(f\"\"\"\n",
    "                    SELECT _record_id, _file_path, _file_name, _file_type, \n",
    "                           _processed_at, data\n",
    "                    FROM unified_data\n",
    "                    WHERE _processed_at BETWEEN '{start_date}' AND '{end_date}'\n",
    "                    ORDER BY _processed_at DESC\n",
    "                \"\"\").fetchdf()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"结构化查询失败: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "\n",
    "    def execute_vector_search(self, query: str, top_k: int = SEARCH_TOP_K) -> List[Dict]:\n",
    "        \"\"\"执行向量相似度搜索\"\"\"\n",
    "        if not self.faiss_index:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # 将查询转换为向量\n",
    "            query_vector = self.text_model.encode([query])[0]\n",
    "            \n",
    "            # 执行相似度搜索\n",
    "            D, I = self.faiss_index.search(np.array([query_vector], dtype='float32'), top_k)\n",
    "            \n",
    "            results = []\n",
    "            for idx, distance in zip(I[0], D[0]):\n",
    "                if idx in self.vectors_map:\n",
    "                    vector_data = self.vectors_map[idx]\n",
    "                    results.append({\n",
    "                        'record_id': vector_data['record_id'],\n",
    "                        'file_name': vector_data['file_name'],\n",
    "                        'file_type': vector_data['file_type'],\n",
    "                        'data': vector_data['data'],\n",
    "                        'similarity': 1 - distance/2  # 转换为相似度分数\n",
    "                    })\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"向量搜索失败: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def enhance_results(self, results: Dict) -> Dict:\n",
    "        \"\"\"增强搜索结果\"\"\"\n",
    "        enhanced = results.copy()\n",
    "        \n",
    "        # 添加统计信息\n",
    "        total = results['stats']['total']\n",
    "        enhanced['summary'] = {\n",
    "            'total_results': total,\n",
    "            'structured_ratio': results['stats']['structured_count'] / total if total > 0 else 0,\n",
    "            'vector_ratio': results['stats']['vector_count'] / total if total > 0 else 0\n",
    "        }\n",
    "        \n",
    "        # 分析文件类型分布\n",
    "        if results['structured']:\n",
    "            df = pd.DataFrame(results['structured'])\n",
    "            if '_file_type' in df.columns:\n",
    "                enhanced['summary']['file_types'] = df['_file_type'].value_counts().to_dict()\n",
    "        \n",
    "        # 添加时间维度分析\n",
    "        if results['structured']:\n",
    "            df = pd.DataFrame(results['structured'])\n",
    "            if '_processed_at' in df.columns:\n",
    "                enhanced['summary']['time_range'] = {\n",
    "                    'earliest': df['_processed_at'].min().isoformat(),\n",
    "                    'latest': df['_processed_at'].max().isoformat()\n",
    "                }\n",
    "        \n",
    "        return enhanced\n",
    "\n",
    "    def format_results(self, results: Dict) -> str:\n",
    "        \"\"\"格式化搜索结果为可读文本\"\"\"\n",
    "        output = []\n",
    "        \n",
    "        # 添加总览信息\n",
    "        output.append(f\"找到 {results['stats']['total']} 条相关结果\")\n",
    "        output.append(f\"其中结构化数据 {results['stats']['structured_count']} 条\")\n",
    "        output.append(f\"向量相似度匹配 {results['stats']['vector_count']} 条\")\n",
    "        \n",
    "        # 添加结构化搜索结果\n",
    "        if results['structured']:\n",
    "            output.append(\"\\n结构化数据匹配:\")\n",
    "            for item in results['structured'][:3]:  # 只显示前3条\n",
    "                output.append(f\"- 文件: {item['_file_name']}\")\n",
    "                output.append(f\"  类型: {item['_file_type']}\")\n",
    "                # 直接使用data字段，不需要额外的JSON解析\n",
    "                data_str = str(item['data'])[:200] + \"...\" if len(str(item['data'])) > 200 else str(item['data'])\n",
    "                output.append(f\"  内容: {data_str}\")\n",
    "        \n",
    "        # 添加向量搜索结果\n",
    "        if results['vector']:\n",
    "            output.append(\"\\n相似内容匹配:\")\n",
    "            for item in results['vector'][:3]:  # 只显示前3条\n",
    "                output.append(f\"- 相似度: {item['similarity']:.2f}\")\n",
    "                output.append(f\"  文件: {item['file_name']}\")\n",
    "                output.append(f\"  类型: {item['file_type']}\")\n",
    "                # 直接使用data字段，不需要额外的JSON解析\n",
    "                data_str = str(item['data'])[:200] + \"...\" if len(str(item['data'])) > 200 else str(item['data'])\n",
    "                output.append(f\"  内容: {data_str}\")\n",
    "        \n",
    "        # 添加统计信息\n",
    "        if 'summary' in results:\n",
    "            output.append(\"\\n统计信息:\")\n",
    "            if 'file_types' in results['summary']:\n",
    "                output.append(\"文件类型分布:\")\n",
    "                for ftype, count in results['summary']['file_types'].items():\n",
    "                    output.append(f\"- {ftype}: {count}\")\n",
    "            \n",
    "            if 'time_range' in results['summary']:\n",
    "                output.append(f\"\\n时间范围:\")\n",
    "                output.append(f\"从 {results['summary']['time_range']['earliest']}\")\n",
    "                output.append(f\"至 {results['summary']['time_range']['latest']}\")\n",
    "        \n",
    "        return \"\\n\".join(output)\n",
    "\n",
    "    def search(self, query: str) -> str:\n",
    "        \"\"\"执行搜索并返回格式化结果\"\"\"\n",
    "        try:\n",
    "            # 1. 解析查询\n",
    "            parsed_query = self.parse_query(query)\n",
    "            \n",
    "            # 2. 执行搜索\n",
    "            structured_results = self.execute_structured_query(parsed_query)\n",
    "            vector_results = self.execute_vector_search(query)\n",
    "            \n",
    "            # 3. 整合结果\n",
    "            results = {\n",
    "                'structured': structured_results.to_dict('records'),\n",
    "                'vector': vector_results,\n",
    "                'stats': {\n",
    "                    'total': len(structured_results) + len(vector_results),\n",
    "                    'structured_count': len(structured_results),\n",
    "                    'vector_count': len(vector_results)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # 4. 增强结果\n",
    "            enhanced_results = self.enhance_results(results)\n",
    "            \n",
    "            # 5. 格式化输出\n",
    "            return self.format_results(enhanced_results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"搜索过程中发生错误: {str(e)}\"\n",
    "\n",
    "# %% [5. 环境验证]\n",
    "def validate_environment():\n",
    "    \"\"\"验证运行环境\"\"\"\n",
    "    try:\n",
    "        import sentence_transformers\n",
    "        \n",
    "        required_packages = {\n",
    "            'duckdb': duckdb.__version__,\n",
    "            'faiss': faiss.__version__,\n",
    "            'pandas': pd.__version__,\n",
    "            'numpy': np.__version__,\n",
    "            'sentence_transformers': sentence_transformers.__version__\n",
    "        }\n",
    "        \n",
    "        print(\"环境检查:\")\n",
    "        for package, version in required_packages.items():\n",
    "            print(f\"- {package}: {version}\")\n",
    "        \n",
    "        if not os.path.exists(DEFAULT_DB_PATH):\n",
    "            print(\"\\n初始化数据库...\")\n",
    "            setup_database()\n",
    "            print(f\"数据库已创建: {DEFAULT_DB_PATH}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"环境检查失败: {str(e)}\")\n",
    "        print(\"请确保已安装所有必要的包:\")\n",
    "        print(\"pip install faiss-cpu duckdb pandas numpy sentence-transformers\")\n",
    "\n",
    "# %% [6. 主程序]\n",
    "if __name__ == \"__main__\":\n",
    "    # 环境检查\n",
    "    validate_environment()\n",
    "    \n",
    "    # 创建搜索引擎实例\n",
    "    engine = SearchEngine()\n",
    "    \n",
    "    # 测试查询\n",
    "    test_queries = [\n",
    "        \"机器学习相关文档\",                    # 普通文本搜索\n",
    "        \"file:json\",                   # 按文件类型搜索\n",
    "        \"date:2025-01-01 to 2025-12-31\"  # 按日期范围搜索\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n开始测试查询...\")\n",
    "    for query in test_queries:\n",
    "        print(f\"\\n查询: {query}\")\n",
    "        print(\"-\" * 50)\n",
    "        result = engine.search(query)\n",
    "        print(result)\n",
    "        print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kachi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

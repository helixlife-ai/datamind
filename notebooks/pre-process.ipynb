{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Helix\\.conda\\envs\\kachi\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-05 00:44:07 - INFO - ==================================================\n",
      "2025-02-05 00:44:07 - INFO - 开始数据预处理任务\n",
      "2025-02-05 00:44:07 - INFO - ==================================================\n",
      "2025-02-05 00:44:07 - INFO - 输入目录: [WindowsPath('D:/github/My/unified-research/source/test_notes')]\n",
      "2025-02-05 00:44:07 - INFO - --------------------------------------------------\n",
      "2025-02-05 00:44:07 - INFO - 开始扫描文件...\n",
      "2025-02-05 00:44:07 - INFO - 开始扫描路径: [WindowsPath('D:/github/My/unified-research/source/test_notes')]\n",
      "2025-02-05 00:44:07 - INFO - 扫描目录: D:\\github\\My\\unified-research\\source\\test_notes\n",
      "2025-02-05 00:44:07 - INFO - 扫描完成，共找到 1 个文件\n",
      "2025-02-05 00:44:07 - INFO - 扫描完成，共发现 1 个文件\n",
      "2025-02-05 00:44:07 - INFO - --------------------------------------------------\n",
      "2025-02-05 00:44:07 - INFO - 初始化解析器和存储系统...\n",
      "2025-02-05 00:44:07 - INFO - 初始化文件解析器\n",
      "2025-02-05 00:44:07 - INFO - 加载文本向量化模型...\n",
      "2025-02-05 00:44:07 - INFO - Use pytorch device_name: cpu\n",
      "2025-02-05 00:44:07 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n",
      "2025-02-05 00:46:10 - INFO - 文本向量化模型加载完成\n",
      "2025-02-05 00:46:10 - INFO - 初始化存储系统，数据库路径: unified_storage.duckdb\n",
      "2025-02-05 00:46:11 - INFO - 初始化数据库表结构\n",
      "2025-02-05 00:46:11 - INFO - 存储表初始化完成\n",
      "2025-02-05 00:46:11 - INFO - --------------------------------------------------\n",
      "2025-02-05 00:46:11 - INFO - 开始处理文件...\n",
      "2025-02-05 00:46:11 - INFO - ==================================================\n",
      "2025-02-05 00:46:11 - INFO - 处理文件 [1/1]: D:\\github\\My\\unified-research\\source\\test_notes\\card_data_副业.json\n",
      "2025-02-05 00:46:11 - INFO - 开始解析文件: D:\\github\\My\\unified-research\\source\\test_notes\\card_data_副业.json\n",
      "2025-02-05 00:46:11 - INFO - 开始解析文件: D:\\github\\My\\unified-research\\source\\test_notes\\card_data_副业.json\n",
      "2025-02-05 00:46:11 - INFO - 文件类型: .json\n",
      "2025-02-05 00:46:11 - INFO - 使用JSON解析器\n",
      "2025-02-05 00:46:11 - INFO - 读取JSON文件: D:\\github\\My\\unified-research\\source\\test_notes\\card_data_副业.json\n",
      "2025-02-05 00:46:11 - INFO - JSON解析完成，获取 7 条记录\n",
      "2025-02-05 00:46:11 - INFO - 成功解析记录数: 7\n",
      "2025-02-05 00:46:11 - INFO - 处理第 1/7 条记录\n",
      "2025-02-05 00:46:11 - INFO - 为记录 1 生成向量表示\n",
      "2025-02-05 00:46:11 - INFO - 开始生成向量表示\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.11it/s]\n",
      "2025-02-05 00:46:11 - INFO - 向量生成完成，维度: 384\n",
      "2025-02-05 00:46:11 - INFO - 记录 1 处理完成，耗时: 0.12秒\n",
      "2025-02-05 00:46:11 - INFO - 处理第 2/7 条记录\n",
      "2025-02-05 00:46:11 - INFO - 为记录 2 生成向量表示\n",
      "2025-02-05 00:46:11 - INFO - 开始生成向量表示\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.17it/s]\n",
      "2025-02-05 00:46:11 - INFO - 向量生成完成，维度: 384\n",
      "2025-02-05 00:46:11 - INFO - 记录 2 处理完成，耗时: 0.07秒\n",
      "2025-02-05 00:46:11 - INFO - 处理第 3/7 条记录\n",
      "2025-02-05 00:46:11 - INFO - 为记录 3 生成向量表示\n",
      "2025-02-05 00:46:11 - INFO - 开始生成向量表示\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.97it/s]\n",
      "2025-02-05 00:46:11 - INFO - 向量生成完成，维度: 384\n",
      "2025-02-05 00:46:11 - INFO - 记录 3 处理完成，耗时: 0.06秒\n",
      "2025-02-05 00:46:11 - INFO - 处理第 4/7 条记录\n",
      "2025-02-05 00:46:11 - INFO - 为记录 4 生成向量表示\n",
      "2025-02-05 00:46:11 - INFO - 开始生成向量表示\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.89it/s]\n",
      "2025-02-05 00:46:11 - INFO - 向量生成完成，维度: 384\n",
      "2025-02-05 00:46:11 - INFO - 记录 4 处理完成，耗时: 0.09秒\n",
      "2025-02-05 00:46:11 - INFO - 处理第 5/7 条记录\n",
      "2025-02-05 00:46:11 - INFO - 为记录 5 生成向量表示\n",
      "2025-02-05 00:46:11 - INFO - 开始生成向量表示\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.40it/s]\n",
      "2025-02-05 00:46:11 - INFO - 向量生成完成，维度: 384\n",
      "2025-02-05 00:46:11 - INFO - 记录 5 处理完成，耗时: 0.06秒\n",
      "2025-02-05 00:46:11 - INFO - 处理第 6/7 条记录\n",
      "2025-02-05 00:46:11 - INFO - 为记录 6 生成向量表示\n",
      "2025-02-05 00:46:11 - INFO - 开始生成向量表示\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.93it/s]\n",
      "2025-02-05 00:46:11 - INFO - 向量生成完成，维度: 384\n",
      "2025-02-05 00:46:11 - INFO - 记录 6 处理完成，耗时: 0.06秒\n",
      "2025-02-05 00:46:11 - INFO - 处理第 7/7 条记录\n",
      "2025-02-05 00:46:11 - INFO - 为记录 7 生成向量表示\n",
      "2025-02-05 00:46:11 - INFO - 开始生成向量表示\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.52it/s]\n",
      "2025-02-05 00:46:11 - INFO - 向量生成完成，维度: 384\n",
      "2025-02-05 00:46:11 - INFO - 记录 7 处理完成，耗时: 0.06秒\n",
      "2025-02-05 00:46:11 - INFO - 文件 card_data_副业.json 所有记录处理完成\n",
      "2025-02-05 00:46:11 - INFO - 文件解析成功，获得 7 条记录\n",
      "2025-02-05 00:46:11 - INFO - 开始存储数据...\n",
      "2025-02-05 00:46:11 - INFO - 准备存储 7 条记录到数据库\n",
      "2025-02-05 00:46:11 - INFO - 处理元数据字段\n",
      "2025-02-05 00:46:11 - INFO - 序列化向量数据\n",
      "2025-02-05 00:46:11 - INFO - 打包其他字段为JSON格式\n",
      "2025-02-05 00:46:11 - INFO - 准备最终存储数据\n",
      "2025-02-05 00:46:11 - INFO - 开始写入数据库\n",
      "2025-02-05 00:46:11 - INFO - 当前数据库记录数: 0\n",
      "2025-02-05 00:46:11 - INFO - ========================================\n",
      "2025-02-05 00:46:11 - INFO - 数据存储完成\n",
      "2025-02-05 00:46:11 - INFO - 总处理时间: 0.09秒\n",
      "2025-02-05 00:46:11 - INFO - 新增记录数: 7\n",
      "2025-02-05 00:46:11 - INFO - 当前总记录数: 7\n",
      "2025-02-05 00:46:11 - INFO - ========================================\n",
      "2025-02-05 00:46:11 - INFO - 文件处理成功，耗时: 0.64秒\n",
      "2025-02-05 00:46:11 - INFO - ==================================================\n",
      "2025-02-05 00:46:11 - INFO - 数据预处理任务完成\n",
      "2025-02-05 00:46:11 - INFO - 总耗时: 124.60秒\n",
      "2025-02-05 00:46:11 - INFO - 总文件数: 1\n",
      "2025-02-05 00:46:11 - INFO - 成功处理: 1\n",
      "2025-02-05 00:46:11 - INFO - 处理失败: 0\n",
      "2025-02-05 00:46:11 - INFO - 总记录数: 7\n",
      "2025-02-05 00:46:11 - INFO - 平均处理时间: 124.60秒/文件\n",
      "2025-02-05 00:46:11 - INFO - ==================================================\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "\"\"\"\n",
    "# 数据预处理脚本\n",
    "此脚本用于扫描用户指定的一个或多个文件目录，解析文件内容，进行向量化，并将结果存储到统一的数据库表中。\n",
    "\"\"\"\n",
    "\n",
    "# %% [1. 环境准备]\n",
    "import os\n",
    "import json\n",
    "import magic\n",
    "import xmltodict\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import logging\n",
    "\n",
    "# 配置日志\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# %% [2. 智能路径扫描]\n",
    "def smart_scanner(source, max_depth=3):\n",
    "    \"\"\"支持多路径和深度控制的扫描器\"\"\"\n",
    "    logger.info(f\"开始扫描路径: {source}\")\n",
    "    found = []\n",
    "    for path in [source] if isinstance(source, (str, Path)) else source:\n",
    "        path = Path(path)\n",
    "        if path.is_file():\n",
    "            found.append(path)\n",
    "            logger.debug(f\"添加文件: {path}\")\n",
    "        elif path.is_dir():\n",
    "            logger.info(f\"扫描目录: {path}\")\n",
    "            for root, dirs, files in os.walk(path):\n",
    "                depth = len(Path(root).relative_to(path).parts)\n",
    "                if depth > max_depth:\n",
    "                    logger.debug(f\"跳过深度 {depth} 的目录: {root}\")\n",
    "                    del dirs[:]\n",
    "                    continue\n",
    "                for f in files:\n",
    "                    if not f.startswith('.'):\n",
    "                        found.append(Path(root)/f)\n",
    "                        logger.debug(f\"添加文件: {f}\")\n",
    "    \n",
    "    logger.info(f\"扫描完成，共找到 {len(found)} 个文件\")\n",
    "    return sorted(found)\n",
    "\n",
    "# %% [3. 文件解析引擎]\n",
    "class FileParser:\n",
    "    def __init__(self):\n",
    "        logger.info(\"初始化文件解析器\")\n",
    "        logger.info(\"加载文本向量化模型...\")\n",
    "        self.text_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "        logger.info(\"文本向量化模型加载完成\")\n",
    "    \n",
    "    def parse(self, file_path):\n",
    "        \"\"\"统一的文件解析入口\"\"\"\n",
    "        file_path = Path(file_path)\n",
    "        suffix = file_path.suffix.lower()\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"开始解析文件: {file_path}\")\n",
    "            logger.debug(f\"文件类型: {suffix}\")\n",
    "            \n",
    "            # 基础元数据\n",
    "            base_metadata = {\n",
    "                '_file_path': str(file_path),\n",
    "                '_file_name': file_path.name,\n",
    "                '_file_type': suffix.lstrip('.'),\n",
    "                '_processed_at': pd.Timestamp.now(),\n",
    "                '_record_id': f\"{file_path.stem}_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "            }\n",
    "\n",
    "            # 解析文件内容\n",
    "            records = self._parse_file(file_path, suffix)\n",
    "            if not isinstance(records, list):\n",
    "                records = [records]\n",
    "            \n",
    "            logger.info(f\"成功解析记录数: {len(records)}\")\n",
    "\n",
    "            # 处理每条记录\n",
    "            processed_records = []\n",
    "            for idx, record in enumerate(records, 1):\n",
    "                logger.info(f\"处理第 {idx}/{len(records)} 条记录\")\n",
    "                start_time = datetime.now()\n",
    "                \n",
    "                record_data = base_metadata.copy()\n",
    "                record_data['_sub_id'] = idx - 1\n",
    "                record_data['_record_id'] = f\"{record_data['_record_id']}_{idx-1}\"\n",
    "                \n",
    "                # 扁平化记录\n",
    "                logger.debug(f\"扁平化记录 {idx}\")\n",
    "                flat_data = self._flatten_record(record)\n",
    "                record_data.update(flat_data)\n",
    "                \n",
    "                # 生成向量\n",
    "                logger.info(f\"为记录 {idx} 生成向量表示\")\n",
    "                vector = self._generate_vector(flat_data)\n",
    "                if vector:\n",
    "                    record_data['vector'] = vector\n",
    "                    logger.debug(f\"记录 {idx} 向量维度: {len(vector)}\")\n",
    "                \n",
    "                process_time = (datetime.now() - start_time).total_seconds()\n",
    "                logger.info(f\"记录 {idx} 处理完成，耗时: {process_time:.2f}秒\")\n",
    "                \n",
    "                processed_records.append(record_data)\n",
    "\n",
    "            logger.info(f\"文件 {file_path.name} 所有记录处理完成\")\n",
    "            return pd.DataFrame(processed_records)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"解析文件 {file_path} 时出错: {str(e)}\", exc_info=True)\n",
    "            return None\n",
    "\n",
    "    def _parse_file(self, file_path, suffix):\n",
    "        \"\"\"根据文件类型选择解析方法\"\"\"\n",
    "        logger.info(f\"开始解析文件: {file_path}\")\n",
    "        logger.info(f\"文件类型: {suffix}\")\n",
    "        \n",
    "        try:\n",
    "            if suffix == '.json':\n",
    "                logger.info(\"使用JSON解析器\")\n",
    "                return self._parse_json(file_path)\n",
    "            elif suffix in ['.csv', '.tsv']:\n",
    "                logger.info(\"使用CSV解析器\")\n",
    "                return self._parse_csv(file_path)\n",
    "            elif suffix in ['.txt', '.md', '.log']:\n",
    "                logger.info(\"使用文本解析器\")\n",
    "                return self._parse_text(file_path)\n",
    "            elif suffix in ['.xlsx', '.xls']:\n",
    "                logger.info(\"使用Excel解析器\")\n",
    "                return self._parse_excel(file_path)\n",
    "            elif suffix in ['.xml']:\n",
    "                logger.info(\"使用XML解析器\")\n",
    "                return self._parse_xml(file_path)\n",
    "            else:\n",
    "                logger.info(\"使用二进制文件解析器\")\n",
    "                return self._parse_binary(file_path)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"解析失败: {str(e)}\", exc_info=True)\n",
    "            raise\n",
    "    def _parse_json(self, path):\n",
    "        \"\"\"JSON文件解析\"\"\"\n",
    "        logger.info(f\"读取JSON文件: {path}\")\n",
    "        try:\n",
    "            with path.open('r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            result = data if isinstance(data, list) else [data]\n",
    "            logger.info(f\"JSON解析完成，获取 {len(result)} 条记录\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"JSON解析错误: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _parse_csv(self, path):\n",
    "        \"\"\"CSV文件解析\"\"\"\n",
    "        logger.info(f\"读取CSV文件: {path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            result = df.to_dict('records')\n",
    "            logger.info(f\"CSV解析完成，获取 {len(result)} 行数据\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"CSV解析错误: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _parse_text(self, path):\n",
    "        \"\"\"文本文件解析\"\"\"\n",
    "        logger.info(f\"读取文本文件: {path}\")\n",
    "        try:\n",
    "            content = path.read_text(encoding='utf-8')\n",
    "            lines = content.splitlines()\n",
    "            result = [{\n",
    "                'content': content,\n",
    "                'char_count': len(content),\n",
    "                'line_count': len(lines)\n",
    "            }]\n",
    "            logger.info(f\"文本解析完成，共 {len(lines)} 行，{len(content)} 字符\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"文本解析错误: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _parse_excel(self, path):\n",
    "        \"\"\"Excel文件解析\"\"\"\n",
    "        logger.info(f\"读取Excel文件: {path}\")\n",
    "        try:\n",
    "            df = pd.read_excel(path)\n",
    "            result = df.to_dict('records')\n",
    "            logger.info(f\"Excel解析完成，获取 {len(result)} 行数据\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Excel解析错误: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _parse_xml(self, path):\n",
    "        \"\"\"XML文件解析\"\"\"\n",
    "        logger.info(f\"读取XML文件: {path}\")\n",
    "        try:\n",
    "            with path.open('r', encoding='utf-8') as f:\n",
    "                data = xmltodict.parse(f.read())\n",
    "            logger.info(\"XML解析完成\")\n",
    "            return [data]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"XML解析错误: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _parse_binary(self, path):\n",
    "        \"\"\"二进制文件解析\"\"\"\n",
    "        logger.info(f\"读取二进制文件: {path}\")\n",
    "        try:\n",
    "            mime = magic.Magic(mime=True)\n",
    "            file_stat = path.stat()\n",
    "            result = [{\n",
    "                'size': file_stat.st_size,\n",
    "                'mime_type': mime.from_file(str(path)),\n",
    "                'modified_time': datetime.fromtimestamp(file_stat.st_mtime)\n",
    "            }]\n",
    "            logger.info(f\"二进制文件解析完成，大小: {file_stat.st_size} 字节\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"二进制文件解析错误: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _flatten_record(self, data):\n",
    "        \"\"\"将记录扁平化为键值对\"\"\"\n",
    "        flat_data = {}\n",
    "        \n",
    "        def flatten(obj, prefix=''):\n",
    "            if isinstance(obj, dict):\n",
    "                for k, v in obj.items():\n",
    "                    key = f\"{prefix}{k}\" if prefix else k\n",
    "                    if isinstance(v, (str, int, float, bool)) or v is None:\n",
    "                        flat_data[key] = v\n",
    "                    elif isinstance(v, (dict, list)):\n",
    "                        flat_data[key] = json.dumps(v, ensure_ascii=False)\n",
    "                        flatten(v, f\"{key}_\")\n",
    "            elif isinstance(obj, list):\n",
    "                for i, item in enumerate(obj):\n",
    "                    flatten(item, f\"{prefix}{i}_\")\n",
    "            elif isinstance(obj, (str, int, float, bool)):\n",
    "                flat_data[prefix.rstrip('_')] = obj\n",
    "        \n",
    "        flatten(data)\n",
    "        return flat_data\n",
    "\n",
    "    def _generate_vector(self, data):\n",
    "        \"\"\"生成向量表示\"\"\"\n",
    "        logger.info(\"开始生成向量表示\")\n",
    "        text_parts = []\n",
    "        \n",
    "        # 提取所有文本内容\n",
    "        for k, v in data.items():\n",
    "            if isinstance(v, str):\n",
    "                text_parts.append(f\"{k}: {v}\")\n",
    "            elif isinstance(v, (int, float, bool)):\n",
    "                text_parts.append(f\"{k}: {str(v)}\")\n",
    "        \n",
    "        # 组合文本并生成向量\n",
    "        text = \" \".join(text_parts)[:512]  # 限制长度\n",
    "        logger.debug(f\"生成的文本长度: {len(text)} 字符\")\n",
    "        \n",
    "        try:\n",
    "            vector = self.text_model.encode(text)\n",
    "            logger.info(f\"向量生成完成，维度: {len(vector)}\")\n",
    "            return vector.tolist()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"向量化失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "# %% [4. 存储系统]\n",
    "class StorageSystem:\n",
    "    def __init__(self, db_path=\"unified_storage.duckdb\"):\n",
    "        self.db_path = db_path\n",
    "        logger.info(f\"初始化存储系统，数据库路径: {self.db_path}\")\n",
    "        self.db = duckdb.connect(self.db_path)\n",
    "        self.init_storage()\n",
    "    \n",
    "    def init_storage(self):\n",
    "        \"\"\"初始化存储表\"\"\"\n",
    "        logger.info(\"初始化数据库表结构\")\n",
    "        self.db.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS unified_data (\n",
    "                _record_id VARCHAR PRIMARY KEY,  -- 唯一标识符\n",
    "                _file_path VARCHAR,             -- 文件路径\n",
    "                _file_name VARCHAR,             -- 文件名\n",
    "                _file_type VARCHAR,             -- 文件类型\n",
    "                _processed_at TIMESTAMP,        -- 处理时间\n",
    "                _sub_id INTEGER,                -- 子记录ID\n",
    "                data JSON,                      -- 原始数据\n",
    "                vector DOUBLE[]                 -- 向量表示\n",
    "            )\n",
    "        \"\"\")\n",
    "        logger.info(\"存储表初始化完成\")\n",
    "    \n",
    "    def save(self, df):\n",
    "        \"\"\"统一的数据存储方法\"\"\"\n",
    "        if df is None or df.empty:\n",
    "            logger.warning(\"收到空数据，跳过存储\")\n",
    "            return\n",
    "\n",
    "        start_time = datetime.now()\n",
    "        logger.info(f\"准备存储 {len(df)} 条记录到数据库\")\n",
    "        \n",
    "        try:\n",
    "            # 提取核心元数据字段\n",
    "            logger.info(\"处理元数据字段\")\n",
    "            meta_columns = ['_record_id', '_file_path', '_file_name', \n",
    "                          '_file_type', '_processed_at', '_sub_id']\n",
    "            \n",
    "            # 处理向量数据\n",
    "            logger.info(\"序列化向量数据\")\n",
    "            vector_data = df['vector'].apply(json.dumps)\n",
    "            \n",
    "            # 将其他列打包为JSON\n",
    "            logger.info(\"打包其他字段为JSON格式\")\n",
    "            other_columns = [col for col in df.columns \n",
    "                           if col not in meta_columns + ['vector']]\n",
    "            logger.debug(f\"额外字段数量: {len(other_columns)}\")\n",
    "            \n",
    "            df['data'] = df[other_columns].apply(lambda x: json.dumps(x.dropna().to_dict()), axis=1)\n",
    "            \n",
    "            # 准备存储数据\n",
    "            logger.info(\"准备最终存储数据\")\n",
    "            df_to_save = df[meta_columns].copy()\n",
    "            df_to_save['data'] = df['data']\n",
    "            df_to_save['vector'] = vector_data\n",
    "            \n",
    "            # 存储数据\n",
    "            logger.info(\"开始写入数据库\")\n",
    "            self.db.register('temp_view', df_to_save)\n",
    "            \n",
    "            # 记录插入前的记录数\n",
    "            pre_count = self.db.execute(\"SELECT COUNT(*) FROM unified_data\").fetchone()[0]\n",
    "            logger.info(f\"当前数据库记录数: {pre_count}\")\n",
    "            \n",
    "            # 执行插入操作\n",
    "            self.db.execute(\"\"\"\n",
    "                INSERT INTO unified_data \n",
    "                SELECT * FROM temp_view\n",
    "                ON CONFLICT (_record_id) DO UPDATE \n",
    "                SET data = EXCLUDED.data,\n",
    "                    vector = EXCLUDED.vector\n",
    "            \"\"\")\n",
    "            \n",
    "            # 记录插入后的记录数\n",
    "            post_count = self.db.execute(\"SELECT COUNT(*) FROM unified_data\").fetchone()[0]\n",
    "            new_records = post_count - pre_count\n",
    "            \n",
    "            self.db.unregister('temp_view')\n",
    "            \n",
    "            # 计算处理时间\n",
    "            process_time = (datetime.now() - start_time).total_seconds()\n",
    "            \n",
    "            # 输出详细的存储统计\n",
    "            logger.info(\"=\"*40)\n",
    "            logger.info(\"数据存储完成\")\n",
    "            logger.info(f\"总处理时间: {process_time:.2f}秒\")\n",
    "            logger.info(f\"新增记录数: {new_records}\")\n",
    "            logger.info(f\"当前总记录数: {post_count}\")\n",
    "            logger.info(\"=\"*40)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"数据存储过程出错: {str(e)}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "# %% [5. 主流程]\n",
    "def main():\n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(\"开始数据预处理任务\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # 指定输入目录\n",
    "    input_dirs = [r\"D:\\github\\My\\unified-research\\source\\test_notes\"]\n",
    "    input_dirs = [Path(d.strip()) for d in input_dirs]\n",
    "    logger.info(f\"输入目录: {input_dirs}\")\n",
    "    \n",
    "    # 扫描文件\n",
    "    logger.info(\"-\"*50)\n",
    "    logger.info(\"开始扫描文件...\")\n",
    "    scanned_files = smart_scanner(input_dirs)\n",
    "    logger.info(f\"扫描完成，共发现 {len(scanned_files)} 个文件\")\n",
    "    \n",
    "    # 初始化解析器和存储系统\n",
    "    logger.info(\"-\"*50)\n",
    "    logger.info(\"初始化解析器和存储系统...\")\n",
    "    parser = FileParser()\n",
    "    storage = StorageSystem()\n",
    "    \n",
    "    # 处理文件\n",
    "    logger.info(\"-\"*50)\n",
    "    logger.info(\"开始处理文件...\")\n",
    "    total_files = len(scanned_files)\n",
    "    successful_files = 0\n",
    "    failed_files = 0\n",
    "    total_records = 0\n",
    "    \n",
    "    for i, file_path in enumerate(scanned_files, 1):\n",
    "        logger.info(\"=\"*50)\n",
    "        logger.info(f\"处理文件 [{i}/{total_files}]: {file_path}\")\n",
    "        file_start_time = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            df = parser.parse(file_path)\n",
    "            if df is not None:\n",
    "                records_count = len(df)\n",
    "                logger.info(f\"文件解析成功，获得 {records_count} 条记录\")\n",
    "                \n",
    "                logger.info(\"开始存储数据...\")\n",
    "                storage.save(df)\n",
    "                \n",
    "                successful_files += 1\n",
    "                total_records += records_count\n",
    "                \n",
    "                file_process_time = (datetime.now() - file_start_time).total_seconds()\n",
    "                logger.info(f\"文件处理成功，耗时: {file_process_time:.2f}秒\")\n",
    "            else:\n",
    "                failed_files += 1\n",
    "                logger.warning(f\"文件处理失败: {file_path}\")\n",
    "        except Exception as e:\n",
    "            failed_files += 1\n",
    "            logger.error(f\"文件处理异常: {file_path}\", exc_info=True)\n",
    "    \n",
    "    # 输出最终统计信息\n",
    "    total_time = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(\"数据预处理任务完成\")\n",
    "    logger.info(f\"总耗时: {total_time:.2f}秒\")\n",
    "    logger.info(f\"总文件数: {total_files}\")\n",
    "    logger.info(f\"成功处理: {successful_files}\")\n",
    "    logger.info(f\"处理失败: {failed_files}\")\n",
    "    logger.info(f\"总记录数: {total_records}\")\n",
    "    logger.info(f\"平均处理时间: {total_time/total_files:.2f}秒/文件\")\n",
    "    logger.info(\"=\"*50)\n",
    "\n",
    "# %% [6. 运行主流程]\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kachi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
